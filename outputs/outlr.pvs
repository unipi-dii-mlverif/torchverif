cruisenn_lr: THEORY
    BEGIN 
        IMPORTING matrices@matrices

	linear0: MatrixMN(3,9) = (:(:2.599972,-1.218454,-1.531018,-2.047804,-2.833315,-0.819771,-1.244836,1.792637,-0.726693:),(:-1.584255,0.438774,0.713033,0.328750,1.185090,0.255226,0.860830,-1.624294,0.327811:),(:3.008755,-1.068491,-1.556537,6.543468,-2.697885,-1.984466,-4.884281,5.901325,-0.316700:):)
	linear_bias0: MatrixMN(1,9) = (:(:3.842371,-5.348813,-6.020685,6.385042,-2.489472,0.060812,-3.260668,-2.044296,-0.274796:):)
	linear2: MatrixMN(9,1) = (:(:-0.615591:),(:-0.016017:),(:-0.082107:),(:0.206060:),(:-0.039356:),(:0.482476:),(:-0.288040:),(:-0.995668:),(:0.001300:):)
	linear_bias2: MatrixMN(1,1) = (:(:-1.154893:):)

	relu(x: real): real = IF x > 0 THEN x ELSE 0.1*x ENDIF

    reluMat(M: Matrix): MatrixMN(rows(M),columns(M))  =
        form_matrix(LAMBDA (i,j:nat): relu(entry(M)(i,j)), rows(M), columns(M));

	x0inreal: TYPE = { r: real | r>= 20 AND r<= 25 }
	x1inreal: TYPE = { r: real | r>= 30 AND r<= 50 }
	x2inreal: TYPE = { r: real | r>= 0 AND r<=2 }


	net(input: Matrix): Matrix =		
		reluMat(input*linear0+linear_bias0)*linear2+linear_bias2


	% @QED result_size proved by federico on Thu, 26 Oct 2023 07:32:12 GMT
	result_size: LEMMA
		length( net( (:(:0,0,0:):) )   ) = 1

	% @QED base_test proved by federico on Thu, 26 Oct 2023 07:35:16 GMT
	value_tester: LEMMA
		entry( net( (:(: 5,5,5 :):) ) )(0,0) = 2

	% @QED network_bounds proved by federico on Thu, 26 Oct 2023 06:47:45 GMT
	network_bounds: THEOREM
		FORALL (x0in: x0inreal,x1in: x1inreal,x2in: x2inreal):
			entry( net( (:(:x0in,x1in,x2in:):) ) )(0,0) > -200

END cruisenn_lr